{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trabalho propôs a criação de um sistema utilizando algoritmos de aprendizagem de máquina para maximizar a métrica de hit ratio. [1](https://www.cloudflare.com/pt-br/learning/cdn/what-is-a-cache-hit-ratio/#:~:text=A%20cache%20hit%20ratio%20is,at%20fulfilling%20requests%20for%20content.)\n",
    "\n",
    "$$\\frac{\\textrm{number of cache hits}}{\\textrm{number of cache hits + number of cache misses}} = {\\textrm{hit ratio}}{\\textrm{    }}(1)$$ \n",
    "\n",
    "<br><br>\n",
    "Os passos do sistema estão descritos no diagrama abaixo:\n",
    "<center><img src=\"images/diagrama.png\"></center>\n",
    "\n",
    "_A. Obtenção dos dados_\n",
    "\n",
    "Para avaliar o desempenho do sistema proposto foi utilizado um conjunto de dados sintéticos gerados utilizando o código utilizado no projeto DeepCache[2](https://github.com/eman-ramadan/deepcache_netai2018). Os dados gerados seguem a distribuição Zipf para simular a popularidade de cada item ao longo do tempo. O conjunto de dados gerados possue duas colunas, Object_ID e request_time. Para o trabalho foi adicionando uma nova coluna ao conjunto de dados chamado size.\n",
    "\n",
    "* **Object_ID**: Número inteiro representando o ID do objeto que está sendo requisitado.\n",
    "* **request_time**: Tempo incremental em segundos representando o momento que o Object_ID foi requisitado.\n",
    "* **size**: Tamanho do Object_ID em bytes seguindo a distribuição normal.\n",
    "\n",
    "Na etapa de pré-processamento são adicionadas novas colunas para serem utilizadas como _features_ nos algoritmos de aprendizagem de máquina. As novas colunas são:\n",
    "* **time**: Conversão da coluna request_time para o formato data e hora, começando em 01/08/2018.\n",
    "* **hour**: Agrupamento da coluna request_time em intervalos de 3600 segundos e atribuido a cada intervalo um valor representando a hora que ocorreu a requisição.\n",
    "* **total_requests_1H**: Janela deslizante de hora em hora incremental com o número de acessos do object_ID.\n",
    "* **mean_requests_2H**: Janela deslizante de duas horas com o valor médio da coluna total_requests_1H.\n",
    "* **count**: Total de acessos que um object_ID recebeu.\n",
    "\n",
    "Por fim na etapa de pré-processamento é criada uma coluna chamada **cache** para classificar a requisição. Caso o valor dela seja 1 significa que o _object_ID_ tem uma maior probilidade de ser um item popular e que já deva estar ou precise estar em _cache_ e caso o valor seja 0 significa que o item tem uma probilidade menor de ser popular e que não precisa ser armazenado na cache.\n",
    "O método adotado para a classificação foi se o _object_ID_ da linha atual aparece pelo menos uma vez nos próximos 15 _frames_ do conjunto de dados.\n",
    "\n",
    "\n",
    "_C. Algoritmos de classificação utilizados_\n",
    "\n",
    "Para este trabalho foi utilizada integralmente a linguagem de programação _Python_.\n",
    "_Python_ é uma linguagem de programação de alto nível, multiparadigma e interpretada. Devido ao fato de ser uma linguagem fácil de ser apredendida e por todo seu ecossistema, tanto de bibliotecas como da comunidade, ela se tornou uma das linguagens mais utilizadas para tarefas de análise de dados, aprendizagem de máquina e inteligência artificial. [2](https://marutitech.com/python-data-science/) [3](https://www.davekuhlman.org/python_book_01.pdf)\n",
    "\n",
    "Os algoritimos de classificação escolhidos para esse trabalho foram: _Random Forest_, _Gradient Boosting_ e _Decision Tree_. Foram utilizados os algoritmos implementados no pacote _Scikit-learn_ neste trabalho. O _Scikit-learn_ é uma biblioteca da linguagem de programação _Python_ que possui vários algoritmos de aprendizagem de máquina para problemas supervisionados e não supervisionados. [4](https://jmlr.org/papers/v12/pedregosa11a.html)\n",
    "\n",
    "\n",
    "_D. Algoritmos de cache utilizados_\n",
    "\n",
    "Para este trabalho foram selecionados dois algoritmos de _cache_: \n",
    "\n",
    "* **_Least Recently Used_ (LRU)**: Remove o último objeto que está a mais tempo sem ser utilizado quando o cache excede o seu tamanho máximo.\n",
    "* **_Least Frequently Used_ (LFU)**: Remoeve o objeto que é menos utilizado baseado em sua frequência. Ele utiliza um contador para cada objeto para contar a frequência de cada objeto, quando o cache excede o seu tamanho máximo ele remove a página menos frequente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos\n",
    "\n",
    "Os experimentos foram dividios em três partes. Primeiramente o treinamento e avaliação dos algoritmos de classificação no conjunto de dados de treinamento com o objetivo de avaliar o algoritmo de classificação com melhor desempenho. Posteriormente, no conjunto de dados de testes, foi avaliada a métrica de _hit ratio_ dos algortimos de _cache_ e dos algoritmos de aprendizagem de máquina em conjunto com os algoritmos de _cache_ com objetivo de saber se existe um ganho de _hit ratio_ na utilização de um algoritmo de classificação antes de rodar o algoritmo de cache e em qual dos algoritmos, LRU ou LFU, esse ganho é maior. Por fim foi avaliado o tempo médio gasto na execução dos algoritmos de _cache_ e dos algoritmos de classificação em conjunto do algoritmo de _cache_ para avaliar a aplicabilidade da técnica utilizando o algoritmo de classificação no mundo real.\n",
    "\n",
    "\n",
    "_A. Métricas_\n",
    "\n",
    "Para avaliar os modelos gerados pelos algoritmos de classificação foram utilizadas métricas calculadas a partir de uma matriz de confusão [5](https://medium.com/@vitorborbarodrigues/m%C3%A9tricas-de-avalia%C3%A7%C3%A3o-acur%C3%A1cia-precis%C3%A3o-recall-quais-as-diferen%C3%A7as-c8f05e0a513c).\n",
    "A matriz de confusão indica em formato de tabela os erros e acertos do modelo. A tabela abaixo demostra um exemplo de uma matriz de confusão: \n",
    "\n",
    "|   | Valor previsto positivo | Valor previsto negativo |\n",
    "|---|---|---|\n",
    "| **Valor real positivo** |  <center>Verdadeiro Positivo<br>(VP)</center> |  <center>Falso Negativo<br>(FN)</center> |\n",
    "| **Valor real negativo** |  <center>Falso Positivo<br>(FP)</center> | <center>Verdadeiro Negativo<br>(VN)</center> |\n",
    "\n",
    "<center> Matriz de confusão</center>\n",
    "\n",
    "\n",
    "* **Verdadeiro Positivo (VP)**: Classificação correta dos valores positivos previstos.\n",
    "* **Falso Negativo (FN)**: Classificação errada dos valores positivos previstos.\n",
    "* **Falso Positivo (FP)**: Classificação errada dos valores negativos previstos.\n",
    "* **Verdadeiro Negativo (VN)**: Classificação correta dos valores negativos previstos.\n",
    "\n",
    "\n",
    "A partir desses valores é calculada as seguintes métricas para avaliação dos modelos:\n",
    "\n",
    "* **Acurácia**: Indica a perfomance geral que o modelo classificou corretamente, sua fórmula é:\n",
    "$$\\frac{{VP} + {VN}}{{VP} + {VN} + {FP} + {FN}}$$ \n",
    "\n",
    "\n",
    "* **Precisão**: Indica a quantidade das classificações positivas que o modelo fez e quantas estão corretas, sua formulá é:\n",
    "$$\\frac{{VP}}{{VP} + {FP}}$$ \n",
    "\n",
    "\n",
    "* **_Recall_**: Indica a quantidade das classificações positivas corretas, sua formulá é:\n",
    "$$\\frac{{VP}}{{VP} + {FN}}$$ \n",
    "\n",
    "\n",
    "* **_F1-Score_**: Média harmônica entre precisão e _recall_, sua formulá é:\n",
    "$$2 * \\frac{{Precisão} * {Recall}}{{Precisão} + {Recall}}$$\n",
    "\n",
    "\n",
    "\n",
    "Posteriormente para avaliar o desemepenho dos modelos em conjunto com os algoritmos de cache no sistema proposto foi utilizado a métrica de _hit ratio_ da equação (1).\n",
    "No fim para avaliar o tempo médio do sistema, foram selecionados 2000 mil requisições do conjunto de teste e medido o tempo de cada requisição no sistema apenas com os algoritmos de cache e posteriormente nos sistemas propostos com os algoritmos de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte prática"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, random\n",
    "from scipy import special\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_datasets/df_ml.csv\")\n",
    "df = df.set_index(\"time\")\n",
    "df['cache'] = df['cache'].astype(int)\n",
    "\n",
    "display(df.head(18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação dos dados em dados de treinamento e dados de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42, shuffle=False)\n",
    "\n",
    "print('Número de observações nos dados de treinamento:', len(train))\n",
    "print('Número de observações nos dados de testes:',len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhendo as _features_ que serão utilizadas pelos modelos para prever a classificação da coluna _cache_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[1:6]\n",
    "print('Features escolhidas:', ', '.join(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, uniques = pd.factorize(train['cache'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução e avaliação dos algoritmos classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "clf_tree.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a previsão da coluna cache no conjunto de dados de testes\n",
    "preds_tree = uniques[clf_tree.predict(test[features])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['cache'], preds_tree, rownames=['Valores reais'], colnames=['Valores previstos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_metricas_classificadores(test, preds_algo):\n",
    "  accuracy = round(accuracy_score(test['cache'], preds_algo), 2) * 100\n",
    "  precision = round(precision_score(test['cache'], preds_algo), 2) * 100\n",
    "  recall = round(recall_score(test['cache'], preds_algo), 4) * 100\n",
    "  f1 = round(f1_score(test['cache'], preds_algo), 2) * 100\n",
    "\n",
    "  print(f\"Acurácia: {accuracy}%\")\n",
    "  print(f\"Precisão: {precision}%\")\n",
    "  print(f\"Recall: {recall}%\")\n",
    "  print(f\"F1-Score: {f1}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcula_metricas_classificadores(test, preds_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':features,'Importância':np.round(clf_tree.feature_importances_,3)})\n",
    "importances = importances.sort_values('Importância',ascending=False).set_index('feature')\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(n_jobs=12, random_state=0)\n",
    "clf_rf.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a previsão da coluna cache no conjunto de dados de testes\n",
    "preds_rf = uniques[clf_rf.predict(test[features])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['cache'], preds_rf, rownames=['Valores reais'], colnames=['Valores previstos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcula_metricas_classificadores(test, preds_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':features,'Importância':np.round(clf_rf.feature_importances_,3)})\n",
    "importances = importances.sort_values('Importância',ascending=False).set_index('feature')\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gbc = GradientBoostingClassifier(random_state=0)\n",
    "clf_gbc.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a previsão da coluna cache no conjunto de dados de testes\n",
    "preds_gbc = uniques[clf_gbc.predict(test[features])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test['cache'], preds_gbc, rownames=['Valores reais'], colnames=['Valores previstos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcula_metricas_classificadores(test, preds_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':features,'Importância':np.round(clf_gbc.feature_importances_,3)})\n",
    "importances = importances.sort_values('Importância',ascending=False).set_index('feature')\n",
    "display(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentre os algoritmos propostos o algoritmo de classificação o que se saiu melhor nos dados de testes foi o algoritmo de _Decision Tree_ em segundo lugar o _Gradient Boosting_ e por fim o _Random Forest_. Embora o _Random Forest_ foi o último colocado ele foi o que apresentou o maior _recall_ dentre os três algoritmos propostos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução e avaliação dos algoritmos de cache com e sem os classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lru import LRU\n",
    "from lfu import LFU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução apenas dos algoritmos de cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_apenas_cache(algoritmo_cache, capacidade_cache, df):\n",
    "  cache = algoritmo_cache(capacity=capacidade_cache)\n",
    "  cache_status = {}\n",
    "  cache_counter = []\n",
    "  c = 0 \n",
    "\n",
    "  events = df['object_ID'].values\n",
    "\n",
    "  for event_key in events:\n",
    "    if event_key not in cache_status:\n",
    "      cache_status[event_key] = {\n",
    "          'found_in_cache': 0,\n",
    "          'n_requested': 0\n",
    "      }\n",
    "\n",
    "    item = cache.get(event_key)\n",
    "    if item != -1:\n",
    "      c += 1\n",
    "      cache_status[event_key]['found_in_cache'] += 1\n",
    "    else:\n",
    "      cache.put(event_key, 1)\n",
    "\n",
    "    cache_counter.append(c)\n",
    "    cache_status[event_key]['n_requested'] += 1\n",
    "    cache_status[event_key]['hit_ratio'] = round(cache_status[event_key]['found_in_cache'] / cache_status[event_key]['n_requested'], 2)\n",
    "\n",
    "  final_hit_ratios = pd.DataFrame(cache_status).transpose().sort_values(\"hit_ratio\", ascending=False)\n",
    "  \n",
    "  \n",
    "  return cache_counter, final_hit_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_LRU, df_hit_ratios_LRU = executa_apenas_cache(LRU, 15, test)\n",
    "found_in_cache_total_LRU = df_hit_ratios_LRU['found_in_cache'].sum()\n",
    "n_requested_total_LRU = df_hit_ratios_LRU['n_requested'].sum()\n",
    "hit_ratio_LRU = round(( found_in_cache_total_LRU / n_requested_total_LRU ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LRU:', hit_ratio_LRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_LFU, df_hit_ratios_LFU = executa_apenas_cache(LFU, 15, test)\n",
    "found_in_cache_total_LFU = df_hit_ratios_LFU['found_in_cache'].sum()\n",
    "n_requested_total_LFU = df_hit_ratios_LFU['n_requested'].sum()\n",
    "hit_ratio_LFU = round(( found_in_cache_total_LFU / n_requested_total_LFU ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LFU:', hit_ratio_LFU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(counter_LRU, label=\"LRU\")\n",
    "plt.plot(counter_LFU, label=\"LFU\")\n",
    "plt.xlabel('Número total de requisições')\n",
    "plt.ylabel('Total de requisições encontradas em cache')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução do sistema proposto(algoritmo de classificação + algoritmo de cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_sistema_proposto(algoritmo_cache, capacidade_cache, df):\n",
    "  cache = algoritmo_cache(capacity=capacidade_cache)\n",
    "  cache_status = {}\n",
    "  cache_counter = []\n",
    "  c = 0 \n",
    "\n",
    "\n",
    "  for i, row in df.iterrows():\n",
    "    event_key = row['object_ID']\n",
    "    can_cache = row['preds']\n",
    "    \n",
    "    if not can_cache:\n",
    "      continue\n",
    "      \n",
    "    if event_key not in cache_status:\n",
    "      cache_status[event_key] = {\n",
    "          'found_in_cache': 0,\n",
    "          'n_requested': 0\n",
    "      }\n",
    "    \n",
    "    item = cache.get(event_key)\n",
    "    if item != -1:\n",
    "      c += 1\n",
    "      cache_status[event_key]['found_in_cache'] += 1\n",
    "    else:\n",
    "      cache.put(event_key, 1)\n",
    "\n",
    "    cache_counter.append(c)\n",
    "    cache_status[event_key]['n_requested'] += 1\n",
    "    cache_status[event_key]['hit_ratio'] = round(cache_status[event_key]['found_in_cache'] / cache_status[event_key]['n_requested'], 2)\n",
    "\n",
    "  final_hit_ratios = pd.DataFrame(cache_status).transpose().sort_values(\"hit_ratio\", ascending=False)\n",
    "  \n",
    "  \n",
    "  return cache_counter, final_hit_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sistema proposto utilizando Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree = test.copy()\n",
    "test_tree['preds'] = preds_tree\n",
    "\n",
    "\n",
    "counter_LRU_DT, df_hit_ratios_LRU_DT = executa_sistema_proposto(LRU, 15,test_tree)\n",
    "found_in_cache_total_LRU_DT = df_hit_ratios_LRU_DT['found_in_cache'].sum()\n",
    "n_requested_total_LRU_DT = df_hit_ratios_LRU_DT['n_requested'].sum()\n",
    "hit_ratio_LRU_DT = round(( found_in_cache_total_LRU_DT / n_requested_total_LRU_DT ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LRU + Decision Tree:', hit_ratio_LRU_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tree = test.copy()\n",
    "test_tree['preds'] = preds_tree\n",
    "\n",
    "\n",
    "counter_LFU_DT, df_hit_ratios_LFU_DT = executa_sistema_proposto(LFU, 15, test_tree)\n",
    "found_in_cache_total_LFU_DT = df_hit_ratios_LFU_DT['found_in_cache'].sum()\n",
    "n_requested_total_LFU_DT = df_hit_ratios_LFU_DT['n_requested'].sum()\n",
    "hit_ratio_LFU_DT = round(( found_in_cache_total_LFU_DT / n_requested_total_LFU_DT ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LFU + Decision Tree:', hit_ratio_LFU_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sistema proposto utilizando Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf = test.copy()\n",
    "test_rf['preds'] = preds_rf\n",
    "\n",
    "\n",
    "counter_LRU_RF, df_hit_ratios_LRU_RF = executa_sistema_proposto(LRU, 15, test_rf)\n",
    "found_in_cache_total_LRU_RF = df_hit_ratios_LRU_RF['found_in_cache'].sum()\n",
    "n_requested_total_LRU_RF = df_hit_ratios_LRU_RF['n_requested'].sum()\n",
    "hit_ratio_LRU_RF = round(( found_in_cache_total_LRU_RF / n_requested_total_LRU_RF ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LRU + Random Forest:', hit_ratio_LRU_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf = test.copy()\n",
    "test_rf['preds'] = preds_rf\n",
    "\n",
    "\n",
    "counter_LFU_RF, df_hit_ratios_LFU_RF = executa_sistema_proposto(LFU, 15, test_rf)\n",
    "found_in_cache_total_LFU_RF = df_hit_ratios_LFU_RF['found_in_cache'].sum()\n",
    "n_requested_total_LFU_RF = df_hit_ratios_LFU_RF['n_requested'].sum()\n",
    "hit_ratio_LFU_RF = round(( found_in_cache_total_LFU_RF / n_requested_total_LFU_RF ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LFU + Random Forest:', hit_ratio_LFU_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sistema proposto utilizando Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbc = test.copy()\n",
    "test_gbc['preds'] = preds_gbc\n",
    "\n",
    "\n",
    "counter_LRU_GBC, df_hit_ratios_LRU_GBC = executa_sistema_proposto(LRU, 15, test_gbc)\n",
    "found_in_cache_total_LRU_GBC = df_hit_ratios_LRU_GBC['found_in_cache'].sum()\n",
    "n_requested_total_LRU_GBC = df_hit_ratios_LRU_GBC['n_requested'].sum()\n",
    "hit_ratio_LRU_GBC = round(( found_in_cache_total_LRU_GBC / n_requested_total_LRU_GBC ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LRU + Gradient Boosting:', hit_ratio_LRU_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbc = test.copy()\n",
    "test_gbc['preds'] = preds_gbc\n",
    "\n",
    "\n",
    "counter_LFU_GBC, df_hit_ratios_LFU_GBC = executa_sistema_proposto(LFU, 15, test_gbc)\n",
    "found_in_cache_total_LFU_GBC = df_hit_ratios_LFU_GBC['found_in_cache'].sum()\n",
    "n_requested_total_LFU_GBC = df_hit_ratios_LFU_GBC['n_requested'].sum()\n",
    "hit_ratio_LFU_GBC = round(( found_in_cache_total_LFU_GBC / n_requested_total_LFU_GBC ) * 100, 2)\n",
    "\n",
    "print('Hit Ratio LFU + Gradient Boosting:', hit_ratio_LFU_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_requested_total_LRU_DT)\n",
    "print(n_requested_total_LFU_DT)\n",
    "print(n_requested_total_LRU_RF)\n",
    "print(n_requested_total_LFU_RF)\n",
    "print(n_requested_total_LRU_GBC)\n",
    "print(n_requested_total_LFU_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.plot(counter_LRU, label=\"LRU\")\n",
    "plt.plot(counter_LFU, label=\"LFU\")\n",
    "\n",
    "plt.plot(counter_LRU_DT, label=\"LRU + DT\")\n",
    "plt.plot(counter_LFU_DT, label=\"LFU + DT\")\n",
    "plt.plot(counter_LRU_RF, label=\"LRU + RF\")\n",
    "plt.plot(counter_LFU_RF, label=\"LFU + RF\")\n",
    "plt.plot(counter_LRU_GBC, label=\"LRU + GB\")\n",
    "plt.plot(counter_LFU_GBC, label=\"LFU + GB\")\n",
    "# plt.title(\"Algoritmos utilizando LRU\")\n",
    "plt.title(\"Comparação dos algoritmos tradicionais e dos sistemas propostos\")\n",
    "plt.xlabel('Número total de requisições', fontsize=12)\n",
    "plt.ylabel('Total de requisições encontradas em cache', fontsize=12)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# plt.plot(counter_LRU, label=\"LRU\")\n",
    "plt.plot(counter_LFU, label=\"LFU\")\n",
    "\n",
    "# plt.plot(counter_LRU_DT, label=\"LRU + DT\")\n",
    "plt.plot(counter_LFU_DT, label=\"LFU + DT\")\n",
    "# plt.plot(counter_LRU_RF, label=\"LRU + RF\")\n",
    "plt.plot(counter_LFU_RF, label=\"LFU + RF\")\n",
    "# plt.plot(counter_LRU_GBC, label=\"LRU + GBC\")\n",
    "plt.plot(counter_LFU_GBC, label=\"LFU + GBC\")\n",
    "plt.title(\"Algoritmos utilizando LFU\")\n",
    "plt.xlabel('Número total de requisições')\n",
    "plt.ylabel('Total de requisições encontradas em cache')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_algoritmo_cache_benchmark(algoritmo_cache, capacidade_cache, df):\n",
    "  rows = []\n",
    "  for i, r in df.reset_index().iterrows():\n",
    "    current_row = df.iloc[i:i+1].copy()\n",
    "\n",
    "    rows.append(current_row)\n",
    "\n",
    "\n",
    "  cache = algoritmo_cache(capacity=capacidade_cache)\n",
    "  cache_status = {}\n",
    "\n",
    "  cache_counter = []\n",
    "  times = []\n",
    "  c = 0\n",
    "\n",
    "  total = len(rows)\n",
    "  for i, current_row in enumerate(rows[:1000]):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    event_key = current_row['object_ID'].iloc[0]\n",
    "\n",
    "    if event_key not in cache_status:\n",
    "      cache_status[event_key] = {\n",
    "          'found_in_cache': 0,\n",
    "          'n_requested': 0\n",
    "      }\n",
    "\n",
    "    item = cache.get(event_key)\n",
    "    if item != -1:\n",
    "      c += 1\n",
    "      cache_status[event_key]['found_in_cache'] += 1\n",
    "    else:\n",
    "      cache.put(event_key, 1)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    dt = end_time - start_time\n",
    "\n",
    "    cache_counter.append(c)\n",
    "    times.append(dt)\n",
    "    \n",
    "  return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "times_LRU = executa_algoritmo_cache_benchmark(LRU, 15, test)\n",
    "times_LFU = executa_algoritmo_cache_benchmark(LFU, 15, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LRU = pd.DataFrame(times_LRU)\n",
    "x_times_LRU[0] = x_times_LRU[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LRU por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LRU = x_times_LRU.describe().transpose()\n",
    "df_times_LRU['algo'] = 'LRU'\n",
    "\n",
    "x_times_LRU.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LFU = pd.DataFrame(times_LFU)\n",
    "x_times_LFU[0] = x_times_LFU[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LFU por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LFU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LFU = x_times_LFU.describe().transpose()\n",
    "df_times_LFU['algo'] = 'LFU'\n",
    "x_times_LFU.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executa_sistema_proposto_benchmark(algoritmo_cache, capacidade_cache, algo_cl, df):\n",
    "  rows = []\n",
    "  for i, r in df.reset_index().iterrows():\n",
    "    current_row = df.iloc[i:i+1].copy()\n",
    "\n",
    "    rows.append(current_row)\n",
    "\n",
    "\n",
    "  cache = algoritmo_cache(capacity=capacidade_cache)\n",
    "  cache_status = {}\n",
    "\n",
    "  cache_counter = []\n",
    "  times = []\n",
    "  c = 0\n",
    "\n",
    "  total = len(rows)\n",
    "  for i, current_row in enumerate(rows[:1000]):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    pred = uniques[algo_cl.predict(current_row[features])]\n",
    "    current_row['preds'] = pred\n",
    "\n",
    "    event_key = current_row['object_ID'].iloc[0]\n",
    "    can_cache = current_row['preds'].iloc[0]\n",
    "\n",
    "    if not can_cache:\n",
    "      continue\n",
    "\n",
    "    if event_key not in cache_status:\n",
    "      cache_status[event_key] = {\n",
    "          'found_in_cache': 0,\n",
    "          'n_requested': 0\n",
    "      }\n",
    "\n",
    "    item = cache.get(event_key)\n",
    "    if item != -1:\n",
    "      c += 1\n",
    "      cache_status[event_key]['found_in_cache'] += 1\n",
    "    else:\n",
    "      cache.put(event_key, 1)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    dt = end_time - start_time\n",
    "\n",
    "    cache_counter.append(c)\n",
    "    times.append(dt)\n",
    "    \n",
    "  return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_tree = test.copy()\n",
    "test_tree['preds'] = preds_tree\n",
    "\n",
    "times_LRU_DT = executa_sistema_proposto_benchmark(LRU, 15, clf_tree, test_tree)\n",
    "times_LFU_DT = executa_sistema_proposto_benchmark(LFU, 15, clf_tree, test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LRU_DT = pd.DataFrame(times_LRU_DT)\n",
    "x_times_LRU_DT[0] = x_times_LRU_DT[0]  / np.timedelta64(1, 's')\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LRU + Decision Tree por requisição\")\n",
    "\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LRU_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LRU_DT = x_times_LRU_DT.describe().transpose()\n",
    "df_times_LRU_DT['algo'] = 'LRU_DT'\n",
    "\n",
    "x_times_LRU_DT.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LFU_DT = pd.DataFrame(times_LFU_DT)\n",
    "x_times_LFU_DT[0] = x_times_LFU_DT[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LFU + Decision Tree por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LFU_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LFU_DT = x_times_LFU_DT.describe().transpose()\n",
    "df_times_LFU_DT['algo'] = 'LFU_DT'\n",
    "\n",
    "x_times_LFU_DT.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rf = test.copy()\n",
    "test_rf['preds'] = preds_rf\n",
    "\n",
    "times_LRU_RF = executa_sistema_proposto_benchmark(LRU, 15, clf_rf, test_tree)\n",
    "times_LFU_RF = executa_sistema_proposto_benchmark(LFU, 15, clf_rf, test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LRU_RF = pd.DataFrame(times_LRU_RF)\n",
    "x_times_LRU_RF[0] = x_times_LRU_RF[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LRU + Random Forest por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LRU_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LRU_RF = x_times_LRU_RF.describe().transpose()\n",
    "df_times_LRU_RF['algo'] = 'LRU_RF'\n",
    "\n",
    "x_times_LRU_RF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LFU_RF = pd.DataFrame(times_LFU_RF)\n",
    "x_times_LFU_RF[0] = x_times_LFU_RF[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LFU + Random Forest por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LFU_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LFU_RF = x_times_LFU_RF.describe().transpose()\n",
    "df_times_LFU_RF['algo'] = 'LFU_RF'\n",
    "\n",
    "x_times_LFU_RF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gbc = test.copy()\n",
    "test_gbc['preds'] = preds_gbc\n",
    "\n",
    "times_LRU_GBC = executa_sistema_proposto_benchmark(LRU, 15, clf_gbc, test_gbc)\n",
    "times_LFU_GBC = executa_sistema_proposto_benchmark(LFU, 15, clf_gbc, test_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LRU_GBC = pd.DataFrame(times_LRU_GBC)\n",
    "x_times_LRU_GBC[0] = x_times_LRU_GBC[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LRU + Gradient Boosting por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LRU_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LRU_GBC = x_times_LRU_GBC.describe().transpose()\n",
    "df_times_LRU_GBC['algo'] = 'LRU_GBC'\n",
    "\n",
    "x_times_LRU_GBC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_times_LFU_GBC = pd.DataFrame(times_LFU_GBC)\n",
    "x_times_LFU_GBC[0] = x_times_LFU_GBC[0]  / np.timedelta64(1, 's')\n",
    "\n",
    "figure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title(\"Desempenho LFU + Gradient Boosting por requisição\")\n",
    "plt.xlabel(\"Total de requisições\")\n",
    "plt.ylabel(\"Tempo em segundos\")\n",
    "plt.plot(x_times_LFU_GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times_LFU_GBC = x_times_LFU_GBC.describe().transpose()\n",
    "df_times_LFU_GBC['algo'] = 'LFU_GBC'\n",
    "\n",
    "x_times_LFU_GBC.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([df_times_LRU, df_times_LFU, df_times_LRU_DT, df_times_LFU_DT,\n",
    "           df_times_LRU_RF, df_times_LFU_RF, df_times_LRU_GBC, df_times_LFU_GBC]).plot.bar(x=\"algo\", y=\"mean\", log=True, figsize=(20, 16))\n",
    "\n",
    "ax.set_title(\"Tempo médio de execução dos algoritmos\", fontsize=18)\n",
    "ax.set_xlabel(\"Sistemas propostos\", fontsize=16)\n",
    "ax.set_ylabel(\"Tempo médio (em log)\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_times_LRU, df_times_LFU, df_times_LRU_DT, df_times_LFU_DT,\n",
    "           df_times_LRU_GBC, df_times_LFU_GBC]).plot.bar(x=\"algo\", y=\"mean\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_time = pd.concat([df_times_LRU, df_times_LFU, df_times_LRU_DT, df_times_LFU_DT,\n",
    "                         df_times_LRU_RF, df_times_LFU_RF, df_times_LRU_GBC, df_times_LFU_GBC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "Neste trabalho foi proposto a utilização de três algoritmos de aprendizagem de máquina para classificação dos dados antes de um algoritmo de cache. A utilização prévia deles demostrou ser uma técnica eficaz para maximar o _hit ratio_ conseguindo classificar de maneira satisfatória a popularidade de um item no futuro e quais itens precisam passar pelo algoritmo de cache.\n",
    "O sistema proposto utilizando _Decision Tree_ + LRU foi o melhor, conseguindo um _hit ratio_ de 96.93% isso devido ao fato do algoritmo de _Decision Tree_ precisar rodar no LRU apenas 44.66% das requisições. Ele se saiu bem também no tempo médio para classificação de cada requisição, levando em média 0.000891s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
